---
title: "Data Science Capstone: Milestone Report"
author: "Joel Chavez Gomez"
date: "14/2/2022"
output:
        html_document:
                toc: true
                toc_float: true
---
<style>
body {
text-align: justify}
title {
text-align: center
}
image {

</style>

```{r setup, cache = TRUE}
library(tm); library(quanteda)
library(tidyverse); library(quanteda.textmodels)
library(stringi); library(tictoc)
```

```{r source-script, eval=FALSE, include=FALSE}
source("Assets/Scripts/GetPreprocessedCorpus.R")
```
## Introduction
**Natural language processing (NLP)** is a subfield of linguistics, computer science,
and artificial intelligence concerned with the interactions between computers
and human language, in particular how to program computers to process and analyze
large amounts of natural language data. The goal is a computer capable of
"understanding" the contents of documents, including the contextual nuances of
the language within them. The technology can then accurately extract information
and insights contained in the documents as well as categorize and organize the
documents themselves.

<center>
![Figure 1: Natural Language Processing](Assets/Images/nlp/Slide1.PNG)
</center>
  
In this project, we will create a Shiny Web App that predicts the following word 
from a set of words as input. In order to accomplish this, there are a series of 
steps we must take:

1. Getting the source for our training dataset  
2. Loading the training data set into our workspace  
2. Preprocess the training dataset  
3. Process the data set into tokens and ngrams  
4. Build a predictor model  
5. Build a Shiny Web App  

### Getting and Loading the Data
The data set was provided from the [coursera Data Science Capstone](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).
The file consists of a zipfile containing a directory called "final" and four subdirectories, 
one for a different language (German, English, Finnish and Russian). For this 
project we will only be using the English dataset.

The packages `tm` was used to load the text files into our workspace as a corpus.
A corpus is a collection of text, which is unstructured data. For our project we
will need to convert the corpus from this unstructured format into a structured
format, in order to analyze the text and build our predictor model.

The text for our corpus is taken from **Plain Text Document** (*.txt*) files that 
contain text from 3 different sources:  

1. Blogs  
2. News  
3. Twitter  

Since the text are from web sources, they will contain special characters and
links that we will need to remove (*"@", "http://...", "www..."*). This will be 
done during the preprocessing steps.

### Preprocessing
Preprocessing is an important part of Natural Language Processing. Full texts are 
preprocessed to improve computational performance and accuracy of text analysis
techniques. Below is a diagram of the most common preprocessing techniques.
  
<center>
![Preprocessing](Assets/Images/nlp/Slide2.PNG)
</center>
    
The Preprocessing for our project consisted of lowercasing, removing symbols and
numbers, profanity filtering and removing stopwords. Then the corpus was tokenized
and a Document Term Matrix was made. The preprocessing was scripted,
and when sourced it creates RDS files for the objects that will be needed during
Exploratory Data Analysis.

### Processing
The preprocessed corpus was loaded and splitted into three corpus objects for
more efficient processing, one for each source (blogs, news, twitter). The corpora
was then tokenized and ngrams were made (unigrams, bigrams, trigrams).
  
<center>
![Processing](Assets/Images/nlp/Slide3.PNG)
</center>

Then the unigrams, bigrams and trigrams tokens were joined into Data Feature 
Matrices.

<center>
![Preprocessing](Assets/Images/nlp/Slide4.PNG)
</center>
  
## Exploratory Data Analysis
Once the data was loaded, preprocessed and processed into structured data, an 
exploratory data analysis was conducted.

Below is a table with the main characteristics of every text document from the
corpus.
```{r eda, echo = FALSE, cache = TRUE, fig.align='center', message=FALSE, warning = FALSE}
library(R.utils); library(kableExtra); library(tidyverse)
library(readtext); library(tictoc); library(quanteda)
textFiles <- list.files("Assets/Dataset/final/en_US", 
                        full.names = TRUE)
documents <- list.files("Assets/Dataset/final/en_US", 
                        full.names = FALSE)
lines <- c(
        countLines(textFiles[1]), 
        countLines(textFiles[2]), 
        countLines(textFiles[3])
)
fileSize <- c(
        round(file.size(textFiles[1])/1048576),
        round(file.size(textFiles[2])/1048576),
        round(file.size(textFiles[3])/1048576)
)
chars <- c(
        nchar(readtext(textFiles[1])), 
        nchar(readtext(textFiles[2])), 
        nchar(readtext(textFiles[3])) 
)
nchars <- chars[c(2, 4, 6)]
filesInfo <- data.frame(documents, lines, nchars, fileSize)
cnames <- c("Documents", "Number of lines",
            "Number of characters", "File size (Mb)")
colnames(filesInfo) <- cnames
filesInfo %>% kbl() %>%
        kable_styling(bootstrap_options = c("striped", "hover"))
```

```{r sampling, echo = FALSE, cache = TRUE, include=FALSE}
tic("Total")
tic("Splitting corpus")
corpus <- readRDS("Assets/RDS/preprocessedCorpus-tm.rds")
corpus <- corpus(corpus)
blogsCorpus <- corpus(corpus[1])
newsCorpus <- corpus(corpus[2])
twitterCorpus <- corpus(corpus[3])
saveRDS(blogsCorpus, "Assets/RDS/blogsCorpus.rds")
saveRDS(newsCorpus, "Assets/RDS/newsCorpus.rds")
saveRDS(twitterCorpus, "Assets/RDS/twitterCorpus.rds")
rm(corpus, newsCorpus, twitterCorpus)
toc()
```

```{r ngrams, echo = FALSE, include = FALSE, cache = TRUE}
rm(chars, cnames, documents, fileSize, lines, nchars, textFiles, filesInfo)
tic("Ngrams Total")
tic("Blogs Total")
tic("Unigrams")

stopwords_remove <- function(x){
  tokens_select(x, c("im", "dont", "cant", "shouldnt", 
                         "wouldnt", "mr", "dr", "st"), selection = "remove")
}
blogsUnigrams <- tokens(blogsCorpus, what = "fastestword", 
                         remove_punct = TRUE, 
                         remove_symbols = TRUE, 
                         remove_url = TRUE, 
                         remove_separators = TRUE, 
                         split_hyphens = TRUE)
blogsUnigrams <- stopwords_remove(blogsUnigrams)
saveRDS(blogsUnigrams, "Assets/RDS/ngrams/blogsUnigrams.rds")
if(file.exists("Assets/RDS/ngrams/blogsUnigrams.rds") == TRUE){
  message("blogsUnigrams.rds saved correctly")
  rm(blogsCorpus)
} else {
  message("error while saving blogsUnigrams.rds")
}

toc()
tic("Bigrams")
blogsBigrams <- tokens_ngrams(blogsUnigrams, n = 2, concatenator = " ")
saveRDS(blogsBigrams, "Assets/RDS/ngrams/blogsBigrams.rds")
if(file.exists("Assets/RDS/ngrams/blogsBigrams.rds") == TRUE){
  message("blogsBigrams.rds saved correctly")
  rm(blogsBigrams)
} else {
  message("error while saving blogsBigrams.rds")
}

toc()
tic("Trigrams")
blogsTrigrams <- tokens_ngrams(blogsUnigrams, n = 3, concatenator = " ")
saveRDS(blogsTrigrams, "Assets/RDS/ngrams/blogsTrigrams.rds")
if(file.exists("Assets/RDS/ngrams/blogsTrigrams.rds") == TRUE){
  message("blogsTrigrams.rds saved correctly")
  rm(blogsTrigrams)
  rm(blogsUnigrams)
} else {
  message("error while saving blogsTrigrams.rds")
}

toc()
toc()
tic("News Total")
tic("Unigrams")
newsCorpus <- readRDS("Assets/RDS/newsCorpus.rds")
newsUnigrams <- tokens(newsCorpus, what = "fastestword", 
                         remove_punct = TRUE, 
                         remove_symbols = TRUE, 
                         remove_url = TRUE, 
                         remove_separators = TRUE, 
                         split_hyphens = TRUE)
newsUnigrams <- stopwords_remove(newsUnigrams)
saveRDS(newsUnigrams, "Assets/RDS/ngrams/newsUnigrams.rds")
if(file.exists("Assets/RDS/ngrams/newsUnigrams.rds") == TRUE){
  message("newsUnigrams.rds saved correctly")
  rm(newsCorpus)
} else {
  message("error while saving newsUnigrams.rds")
}

toc()
tic("Bigrams")
newsBigrams <- tokens_ngrams(newsUnigrams, n = 2, concatenator = " ")
saveRDS(newsBigrams, "Assets/RDS/ngrams/newsBigrams.rds")
if(file.exists("Assets/RDS/ngrams/newsBigrams.rds") == TRUE){
  message("newsBigrams.rds saved correctly")
  rm(newsBigrams)
} else {
  message("error while saving newsBigrams.rds")
}


toc()
tic("trigrams")
newsTrigrams <- tokens_ngrams(newsUnigrams, n = 3, concatenator = " ")
saveRDS(newsTrigrams, "Assets/RDS/ngrams/newsTrigrams.rds")
if(file.exists("Assets/RDS/ngrams/newsTrigrams.rds") == TRUE){
  message("newsTrigrams.rds saved correctly")
  rm(newsTrigrams)
} else {
  message("error while saving newsTrigrams.rds")
}

rm(newsUnigrams)
toc()
toc()

tic("Twitter total")
tic("Unigrams")
twitterCorpus <- readRDS("Assets/RDS/twitterCorpus.rds")
twitterUnigrams <- tokens(twitterCorpus, what = "fastestword", 
                         remove_punct = TRUE, 
                         remove_symbols = TRUE, 
                         remove_url = TRUE, 
                         remove_separators = TRUE, 
                         split_hyphens = TRUE)
twitterUnigrams <- stopwords_remove(twitterUnigrams)
saveRDS(twitterUnigrams, "Assets/RDS/ngrams/twitterUnigrams.rds")
if(file.exists("Assets/RDS/ngrams/twitterUnigrams.rds") == TRUE){
  message("twitterUnigrams.rds saved correctly")
  rm(twitterCorpus)
} else {
  message("error while saving twitterUnigrams.rds")
}

toc()
tic("Bigrams")
twitterBigrams <- tokens_ngrams(twitterUnigrams, n = 2, concatenator = " ")
saveRDS(twitterBigrams, "Assets/RDS/ngrams/twitterBigrams.rds")
if(file.exists("Assets/RDS/ngrams/twitterBigrams.rds") == TRUE){
  message("twitterBigrams.rds saved correctly")
  rm(twitterBigrams)
} else {
  message("error while saving twitterUnigrams.rds")
}
toc()
tic("Trigrams")
twitterTrigrams <- tokens_ngrams(twitterUnigrams, n = 3, concatenator = " ")
saveRDS(twitterTrigrams, "Assets/RDS/ngrams/twitterTrigrams.rds")
if(file.exists("Assets/RDS/ngrams/twitterTrigrams.rds") == TRUE){
  message("twitterTrigrams.rds saved correctly")
  rm(twitterTrigrams)
} else {
  message("error while saving twitterUnigrams.rds")
}
rm(twitterUnigrams, stopwords_remove)
toc()
toc()
toc()
```

```{r join-toks, echo = FALSE, cache = TRUE, include=FALSE}
tic("Processing 2/2")
blogsUnigrams <- readRDS("Assets/RDS/ngrams/blogsUnigrams.rds")
twitterUnigrams <- readRDS("Assets/RDS/ngrams/twitterUnigrams.rds")
newsUnigrams <- readRDS("Assets/RDS/ngrams/newsUnigrams.rds")
unigramTokens <- tokens(blogsUnigrams) + 
  tokens(newsUnigrams) + 
  tokens(twitterUnigrams)
rm(blogsUnigrams, newsUnigrams, twitterUnigrams)
unigrams_dfm <- dfm(unigramTokens)
rm(unigramTokens)

saveRDS(unigrams_dfm, "Assets/RDS/dfm/unigrams_dfm.rds")
rm(unigrams_dfm)

blogsBigrams <- readRDS("Assets/RDS/ngrams/blogsBigrams.rds")
newsBigrams <- readRDS("Assets/RDS/ngrams/newsBigrams.rds")
twitterBigrams <- readRDS("Assets/RDS/ngrams/twitterBigrams.rds")
bigramTokens <- tokens(blogsBigrams) + 
  tokens(newsBigrams) + 
  tokens(twitterBigrams)
rm(blogsBigrams, newsBigrams, twitterBigrams)
bigrams_dfm <- dfm(bigramTokens)
saveRDS(bigrams_dfm, "Assets/RDS/dfm/bigrams_dfm.rds")
rm(bigrams_dfm, bigramTokens)

blogsTrigrams <- readRDS("Assets/RDS/ngrams/blogsTrigrams.rds")
newsTrigrams <- readRDS("Assets/RDS/ngrams/newsTrigrams.rds")
twitterTrigrams <- readRDS("Assets/RDS/ngrams/twitterTrigrams.rds")
trigramTokens <- tokens(blogsTrigrams) + tokens(newsTrigrams) +
  tokens(twitterTrigrams)
rm(blogsTrigrams, newsTrigrams, twitterTrigrams)
trigrams_dfm <- dfm(trigramTokens)
rm(trigramTokens)
saveRDS(trigrams_dfm, "Assets/RDS/dfm/trigrams_dfm.rds")
rm(trigrams_dfm)
toc()
toc()
```

The main features of the corpus are shown in the graphics below.

```{r main-features, echo = TRUE, cache=TRUE, fig.align='center'}
tic("Analysis")
unigrams_dfm <- readRDS("Assets/RDS/dfm/unigrams_dfm.rds")
topUnigrams <- topfeatures(unigrams_dfm)
topUnigrams <- data.frame(topUnigrams)
Unigram <- rownames(topUnigrams)
topUnigrams <- transmute(topUnigrams, Unigram = Unigram, Count = topUnigrams)

cols <- c("#d00000", "#ffba08", "#229631", "#8fe388", "#1b998b", 
          "#3185fc", "#5d2e8c", "#196bde", "#ff7b9c", "#ff9b85")

g <- ggplot(data = topUnigrams, aes(x = reorder(factor(Unigram), -Count), 
                                    y = Count))
g + geom_col(aes(fill = Unigram)) + 
  scale_fill_manual(values = cols) +
  labs(title = "Top 10 Unigrams", x = "Unigram tokens") + 
  theme_minimal()
rm(Unigram, topUnigrams, unigrams_dfm)
```
```{r main-features2, echo = TRUE, cache=TRUE, fig.align='center'}
bigrams_dfm <- readRDS("Assets/RDS/dfm/bigrams_dfm.rds")
bigrams_dfm <- dfm_select(bigrams_dfm, pattern = c("cant wait", "dont know",
                                                   "im going", "dont"), 
                          selection = "remove")
topBigrams <- topfeatures(bigrams_dfm)
topBigrams <- data.frame(topBigrams)
Bigram <- rownames(topBigrams)
topBigrams <- transmute(topBigrams, Bigram = 
                          Bigram, Count = topBigrams)

cols <- c("#d00000", "#ffba08", "#229631", "#8fe388", "#1b998b", 
          "#3185fc", "#5d2e8c", "#196bde", "#ff7b9c", "#ff9b85")

h <- ggplot(data = topBigrams, aes(x = reorder(factor(Bigram), -Count), 
                                    y = Count))
h + geom_col(aes(fill = Bigram)) + 
  scale_fill_manual(values = cols) +
  labs(title = "Top 10 Bigrams", x = "Bigram tokens") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
rm(Bigram, topBigrams, bigrams_dfm)
```

```{r main-features3, echo = TRUE, cache=TRUE, fig.align='center'}
trigrams_dfm <- readRDS("Assets/RDS/dfm/trigrams_dfm.rds")
trigrams_dfm <- dfm_select(trigrams_dfm, pattern = c("cant wait see",
                                                   "dont even know",
                                                   "feel like im", 
                                                   "im pretty sure", 
                                                   "im", "dont", "cant"), 
                          selection = "remove")
topTrigrams <- topfeatures(trigrams_dfm)
topTrigrams <- data.frame(topTrigrams)
Trigram <- rownames(topTrigrams)
topTrigrams <- transmute(topTrigrams, Trigram = 
                          Trigram, Count = topTrigrams)

cols <- c("#d00000", "#ffba08", "#229631", "#8fe388", "#1b998b", 
          "#3185fc", "#5d2e8c", "#196bde", "#ff7b9c", "#ff9b85")

i <- ggplot(data = topTrigrams, aes(x = reorder(factor(Trigram), -Count), 
                                    y = Count))
i + geom_col(aes(fill = Trigram)) + 
  scale_fill_manual(values = cols) +
  labs(title = "Top 10 Trigrams", x = "Trigram tokens") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
rm(Trigram, topTrigrams, trigrams_dfm)
toc()
```
